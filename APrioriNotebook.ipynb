{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Necessary Imports\n",
    "- Declaring configuration type\n",
    "- Declaring constants for configuration keys (used in setting non-method accessible attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, TypeVar, List, Callable, Self, Set, Tuple, FrozenSet\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pandas import read_csv, to_datetime\n",
    "import re\n",
    "from enum import Enum\n",
    "from copy import deepcopy\n",
    "\n",
    "# Declaring generic variable to augment the future classes flexibility\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "# Declaring enum that represents the type of HashTree nodes possible\n",
    "class HashTreeNodeType(Enum):\n",
    "    INTERNAL = 0\n",
    "    LEAF = 1\n",
    "\n",
    "# Fields for https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html#pyspark.SparkConf\n",
    "@dataclass\n",
    "class Configuration:\n",
    "    appName: str\n",
    "    bindAddress: str\n",
    "    bindPort: str\n",
    "    masterUrl: str\n",
    "\n",
    "\n",
    "CONFIG_BIND_ADDRESS_KEY = \"spark.driver.bindAddress\" # The key for the spark url configuration\n",
    "CONFIG_BIND_PORT_KEY = \"spark.ui.port\" # The key for the spark port configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the spark configuration values from the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'appName': 'APriori Example', 'masterUrl': 'local', 'bindAddress': 'localhost', 'bindPort': '4050'}\n"
     ]
    }
   ],
   "source": [
    "configuration_values: Optional[Dict[str, Any]] = None # The template variable that will hold the configuration values\n",
    "\n",
    "with open(\"configuration.json\", \"r\") as configuration_file:\n",
    "    configuration_values = json.loads(configuration_file.read())\n",
    "\n",
    "print(\"FAILED TO LOAD\" if configuration_values == None else configuration_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the spark configuration from the parsed json configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1ebb884a270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = SparkConf()\n",
    "\n",
    "spark_config: Optional[Configuration] = Configuration(**configuration_values)\n",
    "\n",
    "if spark_config == None:\n",
    "    raise ValueError(\"Must supply configuration, or keep defaults\")\n",
    "\n",
    "configuration.setAppName(spark_config.appName)\n",
    "configuration.setMaster(spark_config.masterUrl)\n",
    "configuration.set(CONFIG_BIND_ADDRESS_KEY, spark_config.bindAddress)\n",
    "configuration.set(CONFIG_BIND_PORT_KEY, spark_config.bindPort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating & Creating the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context: Optional[SparkContext] = None\n",
    "\n",
    "if spark_context is not None:\n",
    "    spark_context.stop()\n",
    "\n",
    "spark_context = SparkContext.getOrCreate(conf=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"baskets\" for APriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_df = read_csv(\"./data/supermarket_sales.csv\")\n",
    "\n",
    "## Some Data Augmenting\n",
    "# Adding Month, Year, and Day columns which correspond to the respective Month/Year/Day values of the dates\n",
    "supermarket_df[\"Date\"] = to_datetime(supermarket_df[\"Date\"])\n",
    "supermarket_df[\"Hour\"] = supermarket_df[\"Time\"].map(lambda x: int(x.split(\":\")[0]))\n",
    "supermarket_df[\"Minute\"] = supermarket_df[\"Time\"].map(lambda x: int(x.split(\":\")[1]))\n",
    "\n",
    "# The goal is to form \"baskets\" from the transactions of each month, which we will use for our APriori analysis\n",
    "supermarket_df = supermarket_df.sort_values(by=\"Date\")\n",
    "\n",
    "# The current basket id\n",
    "basket_id = 1\n",
    "\n",
    "# The collection of baskets\n",
    "transactions = {}\n",
    "\n",
    "# The minute threshold\n",
    "minute_threshold = 10\n",
    "\n",
    "# The unique dates, which represent the unique day/month/year values\n",
    "unique_dates = supermarket_df[\"Date\"].unique()\n",
    "\n",
    "# Iterate over each unique date\n",
    "for each_date in unique_dates:\n",
    "\n",
    "    # Find all dates that match the current `each_date`\n",
    "    x = supermarket_df[(supermarket_df[\"Date\"].dt.year == each_date.year) & (supermarket_df[\"Date\"].dt.month == each_date.month) & (supermarket_df[\"Date\"].dt.day == each_date.day)]\n",
    "    \n",
    "    # Find all unique hours for that specific date\n",
    "    unique_hours = set([int(y) for y in x[\"Hour\"].unique()])\n",
    "\n",
    "    # Iterate over all hours that transactions occurred in that date\n",
    "    for each_hour in unique_hours:\n",
    "\n",
    "        # Find all hour transactions that match the selected hour\n",
    "        hour_transactions = x[x[\"Hour\"] == each_hour]\n",
    "\n",
    "        # Find all unique minutes within that hour\n",
    "        unique_minutes = sorted([int(y) for y in hour_transactions[\"Minute\"].unique()])\n",
    "\n",
    "        # Running set of the currently \"basket'd\" transactions\n",
    "        current_found_transaction_ids = set()\n",
    "        for i in range(len(unique_minutes) - 1):\n",
    "\n",
    "            # The current basket\n",
    "            basket = []\n",
    "\n",
    "            # The current minute\n",
    "            curr_minute = unique_minutes[i]\n",
    "\n",
    "            # All minutes that meet the threshold\n",
    "            future_minutes = list(filter(lambda x: x - curr_minute <= minute_threshold, unique_minutes[i + 1:]))\n",
    "\n",
    "            # Add current minute to the minutes to find\n",
    "            total_minutes_to_find = [curr_minute] + future_minutes\n",
    "            for each_minute in total_minutes_to_find:\n",
    "\n",
    "                # Find the record matching to the minute we are looking for, grabbing it's product name\n",
    "                records = x[x[\"Minute\"] == each_minute][\"Product line\"].to_list()\n",
    "\n",
    "                # Find the record matching to the minute we are looking for, extracting it's id to avoid double counting\n",
    "                record_ids = x[x[\"Minute\"] == each_minute].index.to_list()\n",
    "                \n",
    "                # \"extend\" the basket, which just appends all elements, but avoids duplicates\n",
    "                basket.extend([re.match(r\"\\w+\", records[i]).group(0).lower() for i in range(len(records)) if record_ids[i] not in current_found_transaction_ids])\n",
    "\n",
    "                # For each record that meets the criteria of being in the threshold and already not basket'd\n",
    "                for each_record_id in record_ids:\n",
    "                    # Add the found record id to the currently found transaction ids\n",
    "                    current_found_transaction_ids.add(each_record_id)\n",
    "            \n",
    "            # If the basket has stuff in it, then add it to the list of baskets\n",
    "            if len(basket) > 0:\n",
    "                transactions[basket_id] = basket\n",
    "                basket_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the APriori algorithm :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APriori:\n",
    "    \"\"\"\n",
    "    The basic APriori algorithm, without HashTree implementation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.candidates: List[T] = []\n",
    "        self.support_threshold = None\n",
    "        self.baskets: Dict[int, List[T]]\n",
    "\n",
    "    def prune_candidates(self):\n",
    "        \"\"\"\n",
    "        \"Prunes\" candidates from the dataset, what that implies is double-passing over the dataset, to determine each candidate's support throughout\n",
    "        the entire dataset, runs in O(n^2) time, which can be optimized to O(n) through the use of the HashTree data structure\n",
    "\n",
    "        Raises:\n",
    "            Exception: Throws an exception if no candidates currently meet the support threshold, thus avoiding unnecessary iterations\n",
    "        \"\"\"\n",
    "        filtered_candidates = []\n",
    "        for each_candidate_set  in self.candidates: # for each candidate that exists\n",
    "            support_count = 0\n",
    "            setted_candidate = set(each_candidate_set)\n",
    "            for each_basket in self.baskets.values(): # test against all baskets computed\n",
    "                if setted_candidate.issubset(set(each_basket)): # check if the candidate is a subset of the basket\n",
    "                    support_count += 1\n",
    "\n",
    "            if support_count >= self.support_threshold:\n",
    "                filtered_candidates.append(each_candidate_set)\n",
    "        if len(filtered_candidates) == 0:\n",
    "            raise Exception(\"No candidates meet support threshold, terminating\")\n",
    "\n",
    "        self.candidates = filtered_candidates\n",
    "\n",
    "    def extend_candidates(self):\n",
    "        \"\"\"\n",
    "        \"Extends\" the candidates, which means just forming future candidates to analyze from existing candidates which meet\n",
    "        the support threshold\n",
    "\n",
    "        Raises:\n",
    "            Exception: Throws an exception if no candidates are generated, thus we have reached the maximum possible subset size from the transaction dataset possible\n",
    "        \"\"\"\n",
    "        new_candidates = []\n",
    "        candidates_len = range(len(self.candidates))\n",
    "        for i in candidates_len:\n",
    "            for j in candidates_len:\n",
    "                if i != j:\n",
    "                    candidate_a = self.candidates[i]\n",
    "                    candidate_b = self.candidates[j]\n",
    "                    set_a = set(candidate_a)\n",
    "                    set_b = set(candidate_b)\n",
    "                    union_a_b = set_a.union(set_b)\n",
    "                    is_valid = len(union_a_b) == len(set_a) + 1 and len(union_a_b) == len(set_b) + 1 and not list(union_a_b) in new_candidates\n",
    "\n",
    "                    if is_valid:\n",
    "                        new_candidate = set_a.union(set_b)\n",
    "                        new_candidates.append(list(new_candidate))\n",
    "\n",
    "        if len(new_candidates) == 0:\n",
    "            raise Exception(\"Cannot extend candidates further, frequent itemsets have been found\")\n",
    "        \n",
    "        self.candidates = new_candidates\n",
    "    \n",
    "    def set_candidates(self, candidates: List[T]) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the candidates in the APriori class\n",
    "\n",
    "        Args:\n",
    "            candidates (List[T]): The \"candidates\", or the itemsets that we are determining if they are frequent\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated instance\n",
    "        \"\"\"\n",
    "        self.candidates = candidates\n",
    "        return self\n",
    "    \n",
    "    def set_support_threshold(self, support_threshold: int) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the `support_threshold`, which is integral to the usage of APriori, this allows us to determine which \"candidates\" are\n",
    "        currently frequent and infrequent, which is the entire point of APriori.py\n",
    "\n",
    "        Args:\n",
    "            support_threshold (int): The current support threshold\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated instance\n",
    "        \"\"\"\n",
    "        self.support_threshold = support_threshold\n",
    "        return self\n",
    "    \n",
    "    def set_baskets(self, baskets: List[List[T]]) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the \"baskets\", or the values from the transaction database.\n",
    "        The reason we use `deepcopy` is to avoid mutating the actual database, and always having a clean copy available.\n",
    "\n",
    "        Args:\n",
    "            baskets (List[List[T]]): The data parsed fro the transaction database\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated instance\n",
    "        \"\"\"\n",
    "        self.baskets = deepcopy(baskets)\n",
    "        return self\n",
    "    \n",
    "    def run(self) -> List[Set[T]]:\n",
    "        \"\"\"\n",
    "        The engine of the APriori algorithm, runs the proper functionality in sequence, first pruning, then extending.\n",
    "        1) Pruning to trim down the viable candidates\n",
    "        2) Extending the viable candidates\n",
    "\n",
    "        Repeat steps 1 and 2 until we reach a point when we have the frequent itemsets from the transaction database\n",
    "\n",
    "        Returns:\n",
    "            List[Set[T]]: The frequent itemsets\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                self.prune_candidates()\n",
    "                self.extend_candidates()\n",
    "            except:\n",
    "                return self.candidates\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_candidates = set()\n",
    "for each_basket_id in transactions.values():\n",
    "    converted_candidates.update(set(each_basket_id))\n",
    "\n",
    "singleton_candidates = []\n",
    "for each_item in converted_candidates:\n",
    "    singleton_candidates.append([each_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.21 ms ± 183 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "apriori = APriori().set_baskets(transactions).set_candidates(singleton_candidates).set_support_threshold(9)\n",
    "apriori.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can expand and improve the complexity of APriori, by using a Hash-Tree data structure, I will go over the details further down this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTreeNode:\n",
    "    def __init__(self, branching_factor: int, hashing_function: Callable[[List[T]], int], storage_threshold: int, support_threshold: int):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"   \n",
    "        self.storage_threshold = storage_threshold\n",
    "        self.hashing_function = hashing_function\n",
    "        self.branching_factor = branching_factor\n",
    "        self.stored_itemsets: Set[FrozenSet] = set()\n",
    "        self.support_count = 0\n",
    "        self.type = HashTreeNodeType.LEAF\n",
    "        self.children: List[Optional[Self]] = [None] * self.branching_factor\n",
    "        self.support_threshold = support_threshold\n",
    "        self.candidates_met_threshold = False\n",
    "\n",
    "    def add_itemset(self, itemset: FrozenSet[T]):\n",
    "        if self.storage_threshold == len(self.stored_itemsets) and self.type == HashTreeNodeType.LEAF:\n",
    "            #print('overflow')\n",
    "            self.type = HashTreeNodeType.INTERNAL\n",
    "            cloned_itemsets = self.stored_itemsets.copy()\n",
    "            self.stored_itemsets = set()\n",
    "            self.support_count = 0\n",
    "\n",
    "            for each_itemset in cloned_itemsets:\n",
    "                branch = self.hashing_function(each_itemset) % self.branching_factor\n",
    "                \n",
    "                if self.children[branch] is None:\n",
    "                    self.children[branch] = HashTreeNode(self.branching_factor, self.hashing_function, self.storage_threshold, self.support_threshold)\n",
    "                \n",
    "                self.children[branch].stored_itemsets.add(frozenset(each_itemset))\n",
    "                \n",
    "                #print('branch', branch, ' has ', self.children[branch].stored_itemsets)\n",
    "        elif self.type == HashTreeNodeType.INTERNAL:\n",
    "            found_branch = self.hashing_function(itemset) % self.branching_factor\n",
    "\n",
    "            if self.children[found_branch] is not None:\n",
    "                if self.children[found_branch].type == HashTreeNodeType.INTERNAL:\n",
    "                    self.children[found_branch].add_itemset(itemset)\n",
    "                else:\n",
    "                    self.children[found_branch].stored_itemsets.add(itemset)\n",
    "            else:\n",
    "                self.children[found_branch] = HashTreeNode(self.branching_factor, self.hashing_function, self.storage_threshold, self.support_threshold)\n",
    "                self.children[found_branch].stored_itemsets.add(itemset)\n",
    "        else:\n",
    "            self.stored_itemsets.add(itemset)\n",
    "    \n",
    "    def process_itemset(self, itemset: List[T]) -> Tuple[bool, Set[str]]:\n",
    "        if self.type == HashTreeNodeType.INTERNAL:\n",
    "            found_branch = self.hashing_function(itemset) % self.branching_factor\n",
    "            \n",
    "            if self.children[found_branch] is not None:\n",
    "                if self.children[found_branch].type == HashTreeNodeType.LEAF:\n",
    "                    \n",
    "                    leaf_node = self.children[found_branch]\n",
    "                    setted_itemset = set(itemset)\n",
    "                    is_in_children = any([each_child.issubset(setted_itemset) for each_child in leaf_node.stored_itemsets])\n",
    "                    #print('processing branch ', found_branch, ' with items ', self.children[found_branch].stored_itemsets, ' and itemset ', itemset)\n",
    "                    \n",
    "                    if is_in_children:\n",
    "                        self.children[found_branch].support_count += 1\n",
    "                        return (True, leaf_node.stored_itemsets)\n",
    "                        \n",
    "                    return (False, [])\n",
    "                return self.children[found_branch].process_itemset(itemset)\n",
    "        else:\n",
    "            setted_itemset = set(itemset)\n",
    "            is_in_children = any([each_child.issubset(setted_itemset) for each_child in self.stored_itemsets])\n",
    "\n",
    "            if is_in_children:\n",
    "                return (True, self.stored_itemsets)\n",
    "            \n",
    "        return (False, [])\n",
    "                    \n",
    "\n",
    "class HashTree:\n",
    "    def __init__(self, branching_factor: int, hashing_function: Callable[[List[T]], int], node_storage_threshold: int, support_threshold: int):\n",
    "\n",
    "        if branching_factor is None or hashing_function is None:\n",
    "            raise Exception(\"Must define branching factor and hashing function in order to properly use the Hash-Tree data structure\")\n",
    "        \n",
    "        if branching_factor == 0:\n",
    "            raise Exception(\"Cannot have branching factor of 0\")\n",
    "\n",
    "        if node_storage_threshold <= 0:\n",
    "            raise ValueError(\"Invalid `node_storage_threshold` value\")\n",
    "        \n",
    "        if support_threshold <= 0:\n",
    "            raise ValueError(\"Invalid `support_threshold` value\")\n",
    "\n",
    "        self.branching_factor = branching_factor\n",
    "        self.hashing_function = hashing_function\n",
    "        self.root: Optional[HashTreeNode] = None\n",
    "        self.node_storage_threshold = node_storage_threshold\n",
    "        self.support_threshold = support_threshold\n",
    "\n",
    "    def add_itemset(self, itemset: FrozenSet[T]):\n",
    "        if self.root is None:\n",
    "            self.root = HashTreeNode(self.branching_factor, self.hashing_function, self.node_storage_threshold, self.support_threshold)\n",
    "\n",
    "        self.root.add_itemset(itemset)\n",
    "\n",
    "    def process_itemset(self, itemset: List[T]) -> Tuple[bool, Set[FrozenSet]]:\n",
    "        \"\"\"\n",
    "        Processes the given itemset (basket) in the HashTree\n",
    "\n",
    "        Args:\n",
    "            itemset (List[T]): Validate if the pre-supplied candidates are a subset of the itemset supplied\n",
    "\n",
    "        Returns:\n",
    "            Tuple[bool, List[T]]: Tuple representing whether a candidate was found, and the candidates that are potential subsets of the basket\n",
    "        \"\"\"\n",
    "        if self.root is None:\n",
    "            return (False, [])\n",
    "        \n",
    "        return self.root.process_itemset(itemset)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTreeAPriori:\n",
    "    def __init__(self):\n",
    "        self.candidates: List[FrozenSet[T]] = []\n",
    "        self.support_threshold = None\n",
    "        self.baskets: Dict[int, List[T]]\n",
    "        self.branching_factor = None\n",
    "        self.hashing_function = None\n",
    "        self.node_storage_threshold = None\n",
    "        self.hash_tree : Optional[HashTree] = None\n",
    "        self.candidate_supports = {}\n",
    "        self.iteration = 0\n",
    "        \n",
    "        \n",
    "    def create_hash_tree(self) -> HashTree:\n",
    "        if self.support_threshold is None or self.branching_factor is None or self.hashing_function is None or self.node_storage_threshold is None:\n",
    "            raise ValueError(\"Invalid configuration for HashTree instantiation\")\n",
    "        \n",
    "        self.hash_tree = HashTree(self.branching_factor, self.hashing_function, self.node_storage_threshold, self.support_threshold)\n",
    "\n",
    "    def prune_candidates(self) -> Self:\n",
    "        \"\"\"\n",
    "        \"Prunes\" the existing candidate set, using the support threshold to determine whether any candidates in the current set\n",
    "        are not considered \"frequent\" or not\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the pruning step removes all existing candidates, then return the candidates before pruning, as they are the most frequent\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        filtered_candidates = []\n",
    "        for each_basket in self.baskets.values(): # test against all baskets computed\n",
    "            (found, candidates) = self.hash_tree.process_itemset(each_basket)\n",
    "            \n",
    "            if not found:\n",
    "                continue\n",
    "            \n",
    "            for each_candidate in candidates:\n",
    "                self.candidate_supports[each_candidate] += 1\n",
    "                \n",
    "            \n",
    "        for each_candidate, each_candidate_count in self.candidate_supports.items():\n",
    "            if each_candidate_count >= self.support_threshold:\n",
    "                filtered_candidates.append(each_candidate)\n",
    "\n",
    "        if len(filtered_candidates) == 0:\n",
    "            raise Exception(\"No candidates meet support threshold, terminating\")\n",
    "\n",
    "        self.candidates = filtered_candidates\n",
    "        return self\n",
    "\n",
    "    def extend_candidates(self) -> Self:\n",
    "        \"\"\"\n",
    "        Extends the candidates, extension means forming a new candidate set which we will further prune as well.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If it's impossible to further extend the candidate set\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        new_candidates = []\n",
    "        candidates_len = range(len(self.candidates))\n",
    "\n",
    "        for i in candidates_len:\n",
    "            for j in candidates_len:\n",
    "                if i != j:\n",
    "                    candidate_i = self.candidates[i]\n",
    "                    candidate_j = self.candidates[j]\n",
    "                    set_i = set(candidate_i)\n",
    "                    set_j = set(candidate_j)\n",
    "                    union_i_j = set_i.union(set_j)\n",
    "                    is_extended_itemset_valid = len(union_i_j) == len(set_i) + 1 and len(union_i_j) == len(set_j) + 1 and not list(union_i_j) in new_candidates\n",
    "\n",
    "                    if is_extended_itemset_valid:\n",
    "                        new_candidate = set_i.union(set_j)\n",
    "                        new_candidates.append(frozenset(new_candidate))\n",
    "\n",
    "        if len(new_candidates) == 0:\n",
    "            raise Exception(\"Cannot extend candidates further, frequent itemsets have been found\")\n",
    "        \n",
    "        \n",
    "        return self.set_candidates(new_candidates)\n",
    "    \n",
    "    def set_candidates(self, candidates: List[FrozenSet[T]]) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the \"candidates\", or the itemsets which could potentially be frequent itemsets\n",
    "\n",
    "        Args:\n",
    "            candidates (List[T]): The currently computed frequent itemsets\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        self.candidates = candidates[:]\n",
    "        self.candidate_supports = {}\n",
    "        \n",
    "        self.hash_tree_candidates()\n",
    "        return self\n",
    "    \n",
    "    def hash_tree_candidates(self):\n",
    "        \n",
    "        if self.hash_tree is None:\n",
    "            raise ValueError(\"HashTree is not instantiated\")\n",
    "        \n",
    "        for each_candidate in self.candidates:\n",
    "            self.candidate_supports[frozenset(each_candidate)] = 0\n",
    "            self.hash_tree.add_itemset(frozenset(each_candidate))\n",
    "    \n",
    "    def set_support_threshold(self, support_threshold: int) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the \"support threshold\" for the APriori class, this is the threshold by which we consider a itemset \"frequent\" or not\n",
    "\n",
    "        Args:\n",
    "            support_threshold (int): The support threshold we apply to the candidate sets which is involved in the \"pruning\" step\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        self.support_threshold = support_threshold\n",
    "        return self\n",
    "    \n",
    "    def set_baskets(self, baskets: List[List[T]]) -> Self:\n",
    "        \"\"\"\n",
    "        Sets the \"baskets\", or the parsed transaction database, into a list of transactions that we use to measure the\n",
    "        support value of the candidate set\n",
    "\n",
    "        Args:\n",
    "            baskets (List[List[T]]): The parsed transactions\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        self.baskets = deepcopy(baskets)\n",
    "        return self\n",
    "    \n",
    "    def set_branching_factor(self, branching_factor: int = 5) -> Self:\n",
    "        self.branching_factor = branching_factor\n",
    "        return self\n",
    "    \n",
    "    def set_hashing_function(self, hashing_function: Callable[[List[T]], int]) -> Self:\n",
    "        self.hashing_function = hashing_function\n",
    "        return self\n",
    "    \n",
    "    def set_node_storage_threshold(self, node_storage_threshold: int = 10) -> Self:\n",
    "        self.node_storage_threshold = node_storage_threshold\n",
    "        return self\n",
    "    \n",
    "    def run(self) -> List[T]:\n",
    "        \"\"\"\n",
    "        This is the core function that runs the APriori algorithm on the dataset fields set in the instantiation of the APriori class\n",
    "\n",
    "        Returns:\n",
    "            List[T]: The frequent itemsets calculated using the APriori algorithm\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            self.iteration += 1\n",
    "            try:\n",
    "                self.prune_candidates()\n",
    "                self.create_hash_tree()\n",
    "                self.extend_candidates()\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                # todo: figure out why there are duplicates\n",
    "                filtered_candidates = []\n",
    "                return self.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the APriori with HashTree to work, we first must define a hashing function, branching factor, and node_storage_threshold.\n",
    "Let's do just that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_factor = 20\n",
    "hashing_function = lambda x: sum([sum([ord(z) for z in y]) for y in x])\n",
    "node_storage_threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No candidates meet support threshold, terminating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'food', 'health', 'sports'}),\n",
       " frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'electronic', 'health', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'fashion', 'health', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'food', 'health', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'food', 'health', 'home'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'electronic', 'health', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'electronic', 'health', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'fashion', 'health', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'fashion', 'health', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'food', 'home', 'sports'}),\n",
       " frozenset({'food', 'health', 'home'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'sports'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'electronic', 'food', 'health'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'sports'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'fashion', 'food', 'health'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'home', 'sports'}),\n",
       " frozenset({'electronic', 'health', 'home'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'electronic', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'home'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'fashion', 'home', 'sports'}),\n",
       " frozenset({'fashion', 'health', 'home'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'fashion', 'food', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'sports'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'electronic', 'fashion', 'health'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'electronic', 'fashion', 'food'}),\n",
       " frozenset({'electronic', 'fashion', 'home'}),\n",
       " frozenset({'electronic', 'fashion', 'home'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_apriori = HashTreeAPriori().set_support_threshold(9).set_branching_factor(10).set_hashing_function(hashing_function).set_node_storage_threshold(7)\n",
    "new_apriori.create_hash_tree()\n",
    "new_apriori.set_baskets(transactions).set_candidates(singleton_candidates)\n",
    "new_apriori.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APrioriTid(HashTreeAPriori):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.baskets_db: Dict[int, List[T]] = {}\n",
    "\n",
    "    def init_baskets_db(self) -> Self:\n",
    "        for each_basket_id in self.baskets:\n",
    "            self.baskets_db[each_basket_id] = 0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def prune_candidates(self) -> Self:\n",
    "        \"\"\"\n",
    "        \"Prunes\" the existing candidate set, using the support threshold to determine whether any candidates in the current set\n",
    "        are not considered \"frequent\" or not\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the pruning step removes all existing candidates, then return the candidates before pruning, as they are the most frequent\n",
    "\n",
    "        Returns:\n",
    "            Self: The mutated APriori instance\n",
    "        \"\"\"\n",
    "        filtered_candidates = []\n",
    "        basket_removal_ids = []\n",
    "\n",
    "        for (each_basket_id, each_basket) in self.baskets.items(): # test against all baskets computed\n",
    "            (found, candidates) = self.hash_tree.process_itemset(each_basket)\n",
    "            \n",
    "            if not found:\n",
    "                continue\n",
    "\n",
    "            for each_candidate in candidates:\n",
    "                self.candidate_supports[each_candidate] += 1\n",
    "\n",
    "            # APrioriTid implementation, construct \n",
    "            self.baskets[each_basket_id] = [x for x in set().union(y for y in candidates) if x in each_basket]\n",
    "            if len(self.baskets[each_basket_id]) == 0:\n",
    "                basket_removal_ids.append(each_basket_id)\n",
    "\n",
    "        for each_basket_removal_id in basket_removal_ids:\n",
    "            del self.baskets[each_basket_removal_id]\n",
    "            \n",
    "        for each_candidate, each_candidate_count in self.candidate_supports.items():\n",
    "            if each_candidate_count >= self.support_threshold:\n",
    "                filtered_candidates.append(each_candidate)\n",
    "\n",
    "        if len(filtered_candidates) == 0:\n",
    "            raise Exception(\"No candidates meet support threshold, terminating\")\n",
    "\n",
    "        self.candidates = filtered_candidates\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No candidates meet support threshold, terminating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[frozenset({'health', 'sports'}),\n",
       " frozenset({'food', 'sports'}),\n",
       " frozenset({'home', 'sports'}),\n",
       " frozenset({'electronic', 'sports'}),\n",
       " frozenset({'fashion', 'sports'}),\n",
       " frozenset({'health', 'sports'}),\n",
       " frozenset({'food', 'health'}),\n",
       " frozenset({'health', 'home'}),\n",
       " frozenset({'electronic', 'health'}),\n",
       " frozenset({'fashion', 'health'}),\n",
       " frozenset({'food', 'sports'}),\n",
       " frozenset({'food', 'health'}),\n",
       " frozenset({'food', 'home'}),\n",
       " frozenset({'electronic', 'food'}),\n",
       " frozenset({'fashion', 'food'}),\n",
       " frozenset({'home', 'sports'}),\n",
       " frozenset({'health', 'home'}),\n",
       " frozenset({'food', 'home'}),\n",
       " frozenset({'electronic', 'home'}),\n",
       " frozenset({'fashion', 'home'}),\n",
       " frozenset({'electronic', 'sports'}),\n",
       " frozenset({'electronic', 'health'}),\n",
       " frozenset({'electronic', 'food'}),\n",
       " frozenset({'electronic', 'home'}),\n",
       " frozenset({'electronic', 'fashion'}),\n",
       " frozenset({'fashion', 'sports'}),\n",
       " frozenset({'fashion', 'health'}),\n",
       " frozenset({'fashion', 'food'}),\n",
       " frozenset({'fashion', 'home'}),\n",
       " frozenset({'electronic', 'fashion'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_apriori = APrioriTid().set_support_threshold(9).set_branching_factor(10).set_hashing_function(hashing_function).set_node_storage_threshold(7)\n",
    "new_apriori.create_hash_tree()\n",
    "new_apriori.set_baskets(transactions).set_candidates(singleton_candidates).init_baskets_db()\n",
    "new_apriori.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = TypeVar(\"N\")\n",
    "\n",
    "class LinkedListNode:\n",
    "    def __init__(self):\n",
    "        self.next: Optional[Self] = None\n",
    "        self.prev: Optional[Self] = None\n",
    "        self.value: N\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head: Optional[LinkedListNode] = None\n",
    "        self.tail: Optional[LinkedListNode] = None\n",
    "\n",
    "    def add_node(self, value: N) -> Self:\n",
    "        if self.head is None:\n",
    "            self.head = LinkedListNode(value)\n",
    "            self.tail = self.head\n",
    "        else:\n",
    "            self.tail.next = LinkedListNode(value)\n",
    "            self.tail = self.tail.next\n",
    "    \n",
    "\n",
    "class FPGrowthNode:\n",
    "    def __init__(self):\n",
    "        self.name: Optional[str] = None\n",
    "        self.support_count: int = 0 \n",
    "        self.parent: Optional[Self] = None\n",
    "\n",
    "    def set_parent(self, node: Self) -> Self:\n",
    "        self.parent = node\n",
    "        return self\n",
    "\n",
    "class FPGrowthTree:\n",
    "    def __init__(self):\n",
    "        self.root: Optional[FPGrowthNode] = None\n",
    "        self.node_links: Optional[Dict[FrozenSet, LinkedList]] = None\n",
    "\n",
    "    def add_node(self, node: FPGrowthNode) -> Self:\n",
    "        if self.root is None:\n",
    "            self.root = node\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
